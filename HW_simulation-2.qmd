---
title: "Homework 2: A simulation study investigating the bootstrap"
author: "BIOS 731 – Advanced Statistical Computing"
format:
  pdf:
    colorlinks: true
    urlcolor: blue
    include-in-header:
      - text: |
          \usepackage{multirow}
execute:
  echo: true
  warning: false
  message: false
---


```{r, echo = FALSE}
library(tidyverse)
```

## Context and learning objectives

This assignment reinforces ideas in Module 2: **Simulations and Resampling Methods**. We focus specifically on implementing a **large-scale simulation study**, and the assignment also includes components involving the bootstrap, parallelization, Git/GitHub, and project organization.


## Due Date and Submission

Submit (via Canvas) a PDF knitted from a `.Rmd` or `.qmd` file. Your PDF should include the web address of the GitHub repository containing your work for this assignment. **Commits after the due date will cause the assignment to be considered late.**

## Point distribution

```{r, echo = FALSE}
knitr::kable(
  data.frame(
    Problem = c("Problem 0", "Problem 1.1", "Problem 1.2", "Problem 1.3", "Problem 1.4", "Problem 1.5"),
    Points  = c(20, 10, 5, 20, 30, 15)
  ),
  align = c("l", "r")
)
```

## Problem 0 

This "problem" focuses on the structure of your submission, especially the **use of git and GitHub** for reproducibility, **R Projects** to organize your work, **Quarto/R Markdown** to write reproducible reports, **relative paths** to load local files, and reasonable naming conventions for your files.

To that end:

- Create a public GitHub repository and a local R Project. I suggest naming the repo/directory `bios731_hw2_YourLastName` (e.g., `bios731_hw2_wrobel`).
- Push your **entire project folder** to GitHub.
- Submit a PDF knitted from your `.Rmd`/`.qmd` file to Canvas.
  - Your solutions should be implemented in your `.Rmd`/`.qmd` file.
  - Your git commit history should reflect your workflow (i.e., multiple meaningful commits; avoid a single “final” commit with all work).

## Problem 1

**Simulation study.** Your goal in this homework is to plan and execute a well-organized simulation study for multiple linear regression and confidence intervals constructed via both Wald and bootstrap methods.


### Model

Consider the multiple linear regression model


$$Y_i = \beta_0 + \beta_{treatment}X_{i1} + \mathbf{Z_i}^T\boldsymbol{\gamma} + \epsilon_i$$

where we are primarily interested in the treatment effect $\beta_{treatment}$. It is fine to simulate data with no confounders, i.e. $\gamma = 0$.

### Notation 

Notation is defined below: 

- $Y_i$: continuous outcome
- $X_{i1}$: treatment group indicator; $X_{i1}=1$ for treated 
- $\mathbf{Z_i}$: vector of potential confounders
- $\beta_{treatment}$: average treatment effect, adjusting for $\mathbf{Z_i}$
- $\boldsymbol{\gamma}$: vector of regression coefficient values for confounders 
- $\epsilon_i$: errors, we will vary how these are defined

### Simulation goals

In our simulation, we want to 

- Estimate $\beta_{treatment}$ and $se(\hat{\beta}_{treatment})$
  - Evaluate $\beta_{treatment}$ through bias and coverage
  

You will compare **three** methods for constructing a 95% confidence interval for $\hat{\beta}_{treatment}$:

1. Wald confidence intervals (standard model-based approach)
2. Nonparametric bootstrap **percentile** intervals
3. Nonparametric bootstrap **$t$** intervals
  
You will also evaluate **computation time** for each method.

### Simulation design (full factorial)


Evaluate performance across the following factors:

- Sample size: $n \in \{10, 50, 500\}$
- True treatment effect: $\beta_{treatment} \in \{0, 0.5, 2\}$
- Error distribution:
  - Normal errors: $\epsilon_i \sim N(0, 2)$
  - Heavy-tailed errors: $\epsilon_i \sim t_{\nu}$ with $\nu=3$, scaled to have variance 2

**Implementation hint (heavy tails).** If $u \sim t_{\\nu}$ with $\nu>2$, then $\mathrm{Var}(u)=\nu/(\nu-2)$. To match the normal-error condition variance, set

$$
\epsilon_i = u \cdot \sqrt{2\,\frac{\nu-2}{\nu}}.
$$



### Problem 1.1 ADEMP Structure 

Answer the following questions. Use the ADEMP framework explicitly:

- **A (Aim):** What is the goal of the simulation study?
- **D (Data-generating mechanism):** What model and distributions generate the data? What factors vary across scenarios?
- **E (Estimand):** What quantity(ies) are you trying to learn about?
- **M (Methods):** What methods are being evaluated/compared?
- **P (Performance measures):** What metrics summarize performance?

Also answer:

- How many simulation scenarios will you be running (i.e., how many unique combinations in the full factorial design)?

#### My answers:

- **A (Aim):** The aim of this simulation study is to evaluate the finite-sample performance of confidence interval procedures for the treatment effect parameter $\beta_{\text{treatment}}$ in a linear regression model.

Specifically, we assess and compare the bias, coverage probability, and variability of standard error estimates for Wald confidence intervals and two nonparametric bootstrap-based confidence intervals (percentile and bootstrap-t) under varying sample sizes, true treatment effects, and error distributions.

- **D (Data-generating mechanism):** We generate data according to the linear regression model
$$Y_i = \beta_0 + \beta_{\text{treatment}} X_{i1}+\epsilon_i$$
where $X_{i1}$ is a binary treatment indicator, and $\epsilon_i$ are error terms. For simplicity, no additional confounders are included in the data-generating mechanism. We vary the following factors across simulation scenarios:

- Sample size: $n \in \{10, 50, 500\}$

- True treatment effect: $\beta_{\text{treatment}} \in \{0,0.5,2\}$

- Error distribution:

  - Normal errors: $\epsilon_i \sim N(0,2)$
  
  - Heavy-tailed errors: $\epsilon_i \sim t_{\nu}$ with $\nu=3$, scaled to have variance 2
  
- **E (Estimand):** The primary estimand of interest is the treatment effect parameter $\beta_{\text{treatment}}$ in the linear regression model. We also focus on the standard error of the estimator $\hat{\beta}_{\text{treatment}}$.

- **M (Methods):** We evaluate three methods for constructing 95% confidence intervals for $\hat{\beta}_{\text{treatment}}$:

1. Wald confidence intervals based on the standard error from the fitted linear model.

2. Nonparametric bootstrap percentile intervals, which use the empirical distribution of $\hat{\beta}_{\text{treatment}}^*$ from bootstrap resamples.

3. Nonparametric bootstrap $t$ intervals, which standardize the bootstrap estimates using their estimated standard errors from a second level of bootstrapping.

- **P (Performance measures):** We summarize performance using the following metrics:

1. Bias of $\hat{\beta}_{\text{treatment}}$: the average difference between the estimated and true treatment effect across simulations.

2. Coverage probability of the 95% confidence intervals: the proportion of simulations in which the interval contains the true treatment effect.

3. Distribution of the estimated standard error $se(\hat{\beta}_{\text{treatment}})$ across simulations.

4. Computation time for each method, measured in seconds.

- **Number of simulation scenarios:** There are 3 levels of sample size, 3 levels of true treatment effect, and 2 levels of error distribution, resulting in a total of $3 \times 3 \times 2 = 18$ unique simulation scenarios.

### Problem 1.2 nSim 

Based on desired coverage of 95% with Monte Carlo error of no more than 1%, how many simulations ($n_{sim}$) should you perform for **each** simulation scenario? Implement this value of $n_{\text{sim}}$ throughout your simulation study.

#### My answer:
To achieve a Monte Carlo error of no more than 1% for estimating the coverage probability of 95%, we can use the formula for the standard error of a proportion:

$$SE = \sqrt{\frac{p(1-p)}{n_{sim}}},$$
where $p$ is the true coverage probability (0.95 in this case). We want this standard error to be less than or equal to 0.01:
$$
\sqrt{0.95(1-0.95)/n_{sim}}\leq 0.01,
$$
which yields $n_{sim} \geq 475$. To be conservative, we can round up to $n_{sim} = 500$ simulations per scenario to ensure that the Monte Carlo error is within the desired threshold.



### Problem 1.3 Implementation 

For bootstrap $t$, goal is to estimate a $t$ distribution given by

$$t^* = \frac{\hat{\theta}^*-\hat{\theta}}{s_{\hat{\theta}^*}}$$

where

- $\hat{\theta}^*$: estimated parameter value from each bootstrap iteration
- $\hat{\theta}$: parameter estimate from the original sample
- $s_{\hat{\theta}^*}$ standard error estimate from a given bootstrap sample; requires a second level of bootstrapping to construct.

#### My implementation:

```{r echo = FALSE, message = FALSE, warning = FALSE}
source("code/sim_functions.R")
source("code/run_one_scenario.R")

```

#### Parameter choices 

- Use **B = 500** outer bootstrap resamples for the percentile and bootstrap-$t$ intervals.
- For bootstrap-$t$, use **B\_inner = 100** inner bootstrap resamples.
- Construct **95%** confidence intervals for all methods.

*(If you choose different values, justify your choice and discuss the computation/accuracy trade-off.)*

#### Computing + reproducibility requirements

Execute the full simulation study. For full credit, implement the following:

- Well-structured scripts and subfolders following guidance from the `project_organization` lecture
- Use relative file paths to access intermediate scripts and data objects
- Use readable code practices (clear function boundaries, meaningful names, minimal duplication)
- **Parallelize across simulation scenarios**
  
- Save results from each simulation scenario to an intermediate `.Rds` or `.Rda` file in a `data/` subfolder
  - Add these files to `.gitignore` so they are not pushed to GitHub
- Include a `README.md` explaining your workflow
  - Include what files to run, in what order, and how outputs are produced
- Ensure end-to-end reproducibility:
  - I should be able to clone your GitHub repo, open your `.Rproj`, and run the simulation study to regenerate results

### Problem 1.4 Results summary

Create a plot or table summarizing simulation results across scenarios and methods for each of the following:

- Bias of $\hat{\beta}$
- Coverage of the **95% CI** for $\hat{\beta}$
- Distribution of $se(\hat{\beta})$
- Computation time across methods

**Presentation guidance**

- If creating plots, I encourage faceting by at least one design factor (e.g., $n$, error distribution, or true treatment value).
- Include informative captions for each plot/table.
- For coverage plots, consider adding a reference line at 0.95.


### Problem 1.5 Discussion


Interpret the results summarized   in Problem 1.4.

1. Write **one paragraph** summarizing the main findings of your simulation study.
2. Then answer the questions below:

- How do the different methods for constructing confidence intervals compare in terms of computation time?
- Which method(s) provide the best coverage when $\epsilon_i \sim N(0, 2)$?
- Which method(s) provide the best coverage for the heavy-tailed errors?


Finally, briefly comment on any notable interactions (e.g., how performance changes with $n$ or error type) and any practical recommendations you would make based on your results.
